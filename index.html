<!DOCTYPE HTML>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Haotian Wu</title>
  
  <meta name="author" content="Haotian Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/imperial.JPG">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr style="padding:0px">
    <td style="padding:0px">
	
	<!--Profiles-->	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		
		<tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haotian Wu</name>
              </p>
              <p>
			  Hello! My name is Haotian Wu (Davidwu). I am now an algorithm engineer in Huawei Research Institute,<a href="https://www.huaweicloud.com/intl/en-us/"> Cloud & AI Group</a>. My work now focuses on designing algorithms for practical problems in computer vision and machine learning.
              </p>
			  <p>
                I got my <strong> Master of Science </strong> degree at <a href="http://www.zju.edu.cn/english/">Zhejiang University</a> with <strong> Excellent Postgraduate Students' award </strong> in 2020, where I was supervised by <a href="https://scholar.google.com/citations?hl=en&user=rgwDYosAAAAJ&view_op=list_works&sortby=pubdate"> Prof. Ji Xiang</a>. 
				</p>
				<p>
				Before, I was also awarded the <strong> degree of Master with Distinction</strong> in Control Systems at <a href="https://www.imperial.ac.uk/electrical-engineering/research/control-and-power/">Imperial College London</a> in 2018, where I was supervised by <a href="https://www.imperial.ac.uk/people/r.vinter">Prof. Richard Vinter</a>. 

			   <p>
				I got my <strong> Bachelor's degree</strong> in Automation as <strong>Outstanding Graduates</strong> at <a href="http://www.zju.edu.cn/english/">Zhejiang Univerisity</a> in 2017, where I was supervised by <a href="https://scholar.google.com/citations?user=ic9y2dIAAAAJ&hl=zh-CN">Prof. Zhiyun Lin</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:haotianwu@zju.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/CV_wht.pdf">CV</a> &nbsp/&nbsp
                <a href="data/haotianwu-bio.txt">Biography</a>&nbsp/&nbsp
				<a href="http://eedavidwu.github.io">Blog</a> &nbsp/&nbsp
        <a href="https://github.com/eedavidwu/">Github</a> &nbsp
                <!--<a href="http://www.linkedin.com/in/jonathanbarron/"> LinkedIn </a>-->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
			   <img style="width:100%;max-width:100%" alt="profile photo" src="images/me_circle.jpg" class="hoverZoomLink">

              <!-- <a href="images/My_cat.jpg"> <img style="width:100%;max-width:100%" alt="profile photo" src="images/me_circle.jpg" class="hoverZoomLink"></a> -->
            </td>
			
			 
          </tr>
		  </tbody>
        </table>

        

<!--Research-->		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>
        <tr  bgcolor="#FFF8D7">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in control theory, computer vision, machine learning and robotics. Some of my past research focuses on the visual part of robotics such like visual odometry and depth prediction in stereo vision, etc. Recently, I am doing some work based on the GAN and Siamese net in change detection.
                </p>
                <p>
        Besides, I have passion in the Bayesian computational methods, Monte-Carlo class solution in tracking problems and Markov Decision Process in learning control.
				</p>
				<p>Some of my representative past works are:
              </p>
            </td>
        </tr>
       </tbody>
	   </table>

	<!--Papers-->			
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>
	<!--CD paper-->
		<tr onmouseout="CD_stop()" onmouseover="CD_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='CD'><img src='images/change_st.jpg'></div>
                <img src='images/change.jpg'>
              </div>
              <script type="text/javascript">
                function CD_start() {
                  document.getElementById('CD').style.opacity = "1";
                }

                function CD_stop() {
                  document.getElementById('CD').style.opacity = "0";
                }
                USV_stop()
				
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle><font color='#0066CC'>Change detection with GAN and Siamese net</font></papertitle>

              <br>
              <strong>Haotian Wu</strong>
              <br>
              <a href="https://github.com/eedavidwu/GAN_based_CD">pytorch code</a>
              <p>
				For change detection in remote sensing image, limited dataset, low resolution and poor generalization are main challenges. So GAN and some unsupervised methods attract my interests.
			  </p>
			  <p> 
			  This work algorithm used a Siamese net as the generator, which consists of four encoder and decoder blocks with skip connection. GAN structure is used to encode some latent features and prior knowledge for generalization.			 
			  </p>      
		   </td>
         </tr>      
		   
		<!--Fishery paper-->
		<tr onmouseout="fish_stop()" onmouseover="fish_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fish_image'><img src='images/AUV_fish.jpg'></div>
                <img src='images/fish_new.jpg'>
              </div>
              <script type="text/javascript">
                function fish_start() {
                  document.getElementById('fish_image').style.opacity = "1";
                }

                function fish_stop() {
                  document.getElementById('fish_image').style.opacity = "0";
                }
                fish_stop()
				
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle><font color='#0066CC'>Fishery monitoring system with AUV based on YOLO and SGBM</font></papertitle>

              <br>
              <strong>Haotian Wu</strong>, Shimin He, Zejun Deng, et al.
              <br>
              <em>The 38th Chinese Control Conference (CCC) </em>, 2019
			  <br>
              <a href="https://ieeexplore.ieee.org/document/8866087">Paper</a> /
              <a href="https://youtu.be/Pb2Nfup71D0">video</a> /
              <a href="https://github.com/eedavidwu/Fishery_monitoring">YOLO tensorflow code</a> /
              <a href="https://github.com/eedavidwu/yolov3">YOLO darknet code</a> /
              <a href="https://github.com/eedavidwu/HED_pytorch">HED-net pytorch code</a> 
              <p>
			  This fishery monitoring system detects the fishes by YOLOv3 and predicts the depth map with SGBM to estimate the size of the fishes.
			  </p>
			  <p> 
			  This system is efficient, real-time and accurate compared with the manual method or dual-frequency sonar.
			  </p>
            </td>
         </tr>
		 
		 <!--filter paper-->
		 <tr onmouseout="filter_stop()" onmouseover="filter_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='filter_image'><img src='images/bearing_range_error.jpg'></div>
                <img src='images/bearing_range_tracking.jpg'>
              </div>
              <script type="text/javascript">
                function filter_start() {
                  document.getElementById('filter_image').style.opacity = "1";
                }

                function filter_stop() {
                  document.getElementById('filter_image').style.opacity = "0";
                }
                filter_stop()
				
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
             
                <papertitle><font color='#0066CC'>Hypothetical Analytic Filter for the tracking with bearing and range mixture measurements</font></papertitle>

              <br>
              <strong>Haotian Wu</strong>, Ji Xiang.
              <br>
              <em>The 31th Chinese Control and Decision Conference (CCDC) </em>, 2019
              <br>
              <a href="https://ieeexplore.ieee.org/document/8832333">Paper</a> /
              <a href="https://github.com/eedavidwu/Hypothetical_Analytic_Filter">Code</a> 
              <p>
			  This nonlinear filter uses the bearing information first to overcome the bimodal problem, and then use the range information to correct the performance. 
			 </p>
			  <p>
			  The lower computation demand and its robust convergence are two main advantages.
			  </p>
            </td>
         </tr>
		<!--3rd sensor paper
		 <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/stereo.jpg'>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">

                <papertitle><font color='#0066CC'>Dry camera calibration for underwater stereo processing by semi-global matching</font></papertitle>
        
              <br>
              Zejun Deng, Zhifeng Sun, <strong>Haotian Wu</strong>, et al.
              <br>
              <em>Sensors </em>, 2019
              <br>
              <p>
			  This paper presents an efficient underwater stereo matching algorithm with a dry camera calibration based on the characteristics of underwater image. 

			 </p>
			 <p>

				Experiments show that this method are feasible. This shows that our work for underwater operations has a good guiding significance.			 </p>
            </td>
         </tr>
		 -->
		 
		 
		<!--4th ccc cnn paper-->
		 <tr onmouseout="wind_stop()" onmouseover="wind_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wind_image'><img src='images/wind_pre.jpg'></div>
                <img src='images/wind_result.jpg'>
              </div>
              <script type="text/javascript">
                function wind_start() {
                  document.getElementById('wind_image').style.opacity = "1";
                }

                function wind_stop() {
                  document.getElementById('wind_image').style.opacity = "0";
                }
                wind_stop()
				
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><font color='#0066CC'>A Novel Method for Detection of Wind Turbine Blade Imbalance Based on Multi-Variable Spectrum Imaging and Convolutional Neural Network</font></papertitle>
              
              <br>
              Zhe Cao, Jian Xu, Wei Xiao, Yanjing Gao and <strong>Haotian Wu</strong>.
              <br>
              <em>The 38th Chinese Control Conference (CCC)</em>, 2019
              <br>
              <a href="https://ieeexplore.ieee.org/document/8865600">Paper</a> 
              
      <p>
			This paper presents a novel method used for detecting wind turbine blade imbalance with high accuracy.
			</p>
<p>
			Generator speed, torque and X acceleration were processed into combined spectral images with 'Fourier transform', which were then fed into CNN model to extract the fault features. 
</p>

			 
            </td>
      </tr>
		
		
		<!--5th paper mobile robot-->
		 <tr onmouseout="car_stop()" onmouseover="car_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='car_image'><img src='images/control_all.jpg'></div>
                <img src='images/mobile_QR.jpg'>
              </div>
              <script type="text/javascript">
                function car_start() {
                  document.getElementById('car_image').style.opacity = "1";
                }

                function car_stop() {
                  document.getElementById('car_image').style.opacity = "0";
                }
                car_stop()
				
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><font color='#0066CC'>Self-correcting mobile robot based on special landmark and fuzzy PID control.</font> </papertitle>
              </a>
              <br>
              <strong>Haotian Wu</strong>, Junjie Liu and Kaijian Liu.
              <br>
              <em>Industrial Control Computer </em>, 2017
              <br>
              <a href="https://youtu.be/rOF4JZ1-VXM">video</a> /
			  <a href="https://github.com/eedavidwu/Mobile_robot_interface">project code</a>
              <p>
			  A kind of mobile robot applied to warehouse automation is introduced.  The robot is based on special landmark and fuzzy PID controller.
			 </p>
			  <p>
			  The special landmark-based correction method is faster and more precise than the simple angle-based tracking algorithm. 
			  </p>
            </td>
         </tr>
		
		
		
			<!--6th paper algo-robot-->
		 <tr onmouseout="al_car_stop()" onmouseover="al_car_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='al_car_image'><img src='images/algri_robot.jpg'></div>
                <img src='images/rf.jpg'>
              </div>
              <script type="text/javascript">
                function al_car_start() {
                  document.getElementById('al_car_image').style.opacity = "1";
                }

                function al_car_stop() {
                  document.getElementById('al_car_image').style.opacity = "0";
                }
                al_car_stop()
				
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><font color='#0066CC'>Research on agricultural mobile robot based on random forest. </font> </papertitle>
              </a>
              <br>
              <strong>Haotian Wu</strong>, Qianying Ye and Kaijian Liu.
              <br>
              <em>Industrial Control Computer </em>, 2017
              <p>
	          This mobile robot used QR code as the localization  method. And random forest algorithm is used to predict the growth of crops and make decisions on agricultural behaviors.
	         </p>
            </td>
         </tr>
		
		
		
		</tbody>  
		</table>


<p>
<br>
</p>

<!--Project -->
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>
            <tr bgcolor="#FFF8D7">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Projects</heading>
              <p>
                During my bachelor and master study period time, I did many projects in robotics and computer vision. 
				</p>
				<p>
				Representative projects are:
              </p>
            </td>
          </tr>
		  </tbody>
		</table>
        

	<!--Projetcs_list-->		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>


    <!--1st- Project: defect-->
    <tr onmouseout="defect_stop()" onmouseover="defect_start()" bgcolor="#ECF5FF">
    
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='defect_image'><img src='images/defect_dec_show.jpg'></div>
                <img src='images/defect_dec_net.jpg'>
              </div>
              <script type="text/javascript">
                function defect_start() {
                  document.getElementById('defect_image').style.opacity = "0";
                }

                function defect_stop() {
                  document.getElementById('defect_image').style.opacity = "1";
                }
                defect_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle><font color='#0066CC'>Semi-supervised industrial inspection of tire quality </font></papertitle>
              <br>
              <strong>Haotian Wu</strong>
              <br>
              <em>Huawei Research Institute</em>, &nbsp; 2020.04 - 2020.10
              <br>
              <a href="https://github.com/eedavidwu/defect_detc_GAN">project code</a>
              <p>
        To replace manual detection with AI in the industrial inspection of tire quality, unbalanced datasets and lots of detection noises are main challenges. 
        </p>
              <p>

        I develop a novel GAN-based Semi-supervised method, which only use the positive dataset to detect the defect. Also meta-learning and few shot learning are used for new version.
        </p>
        
            </td>
          </tr>
		
		<!--1st- Project: SLAM and USV-->
		<tr onmouseout="USV_stop()" onmouseover="USV_start()" bgcolor="#ECF5FF">
		
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='visual_usv_img'><img src='images/ORB_SLAM.jpg'></div>
                <img src='images/USV.jpg'>
              </div>
              <script type="text/javascript">
                function USV_start() {
                  document.getElementById('visual_usv_img').style.opacity = "1";
                }

                function USV_stop() {
                  document.getElementById('visual_usv_img').style.opacity = "0";
                }
                USV_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle><font color='#0066CC'>Visual Intelligence of USV</font></papertitle>
              <br>
              <strong>Haotian Wu</strong>,  &nbsp; Feiyang Suo
              <br>
              <em>Zhejiang University</em>, &nbsp; 2019.06 - 2020.03
              <br>
              <a href="https://github.com/eedavidwu/">project code</a>
              <p>
			  I apply the ORB-SLAM into the USV, and use the lidar and IMU to help with the control of the USV.
			  </p>
			  
              <p>
			  For the application, I use a binocular camera as USV's perception method to compute the depth between the camera and the target object, and infer the semantic information.
			  </p>
            </td>
          </tr>
		  
		      <!--Stereo paper-->
		<tr onmouseout="stereo_stop()" onmouseover="stereo_start()" bgcolor="#ECF5FF">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='USV_image'><img src='images/USV_edge_stereo.jpg'></div>
                <img src='images/USV_edge_stereo.jpg'>
              </div>
              <script type="text/javascript">
                function stereo_start() {
                  document.getElementById('USV').style.opacity = "1";
                }

                function stereo_stop() {
                  document.getElementById('USV').style.opacity = "0";
                }
                stereo_stop()
				
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle><font color='#0066CC'>Learning stereo vision and edge extraction for the USV</font></papertitle>

              <br>
              <strong>Haotian Wu</strong>
              <br>
              <a href="https://github.com/eedavidwu/PSM_USV">Stereo vision code</a> /
              <a href="https://github.com/eedavidwu/HED_pytorch">HED-net pytorch code </a> 
              <p>
			  This learning algorithm used a motified PSM and HED net, which results in an end-to-end method to extract the edge information and disparity. 
			  </p>
			  <p> 
			  This algorithm extract the feature with the PSM backbone, and then regress the disparity. The dataset we used is the SceneFlow and KITTI 2015.
			  </p>
            </td>
         </tr>   
		  
		
		<!--2nd- Project: worker-->
		<tr onmouseout="worker_stop()" onmouseover="worker_start()" bgcolor="#ECF5FF">
		
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='worker'><img src='images/heat_map.jpg'></div>
                <img src='images/worker.jpg'>
              </div>
              <script type="text/javascript">
                function worker_start() {
                  document.getElementById('worker').style.opacity = "1";
                }

                function worker_stop() {
                  document.getElementById('worker').style.opacity = "0";
                }
                worker_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle><font color='#0066CC'>Safety monitoring of construction scene</font></papertitle>

              <br>
              <strong>Haotian Wu</strong>
              <br>
              <em>Huawei Research Institute, Cloud & AI Group</em>,  &nbsp; 2019.06 - 2019.09
              <br>
              <a href="https://github.com/eedavidwu/yolov3_notes">Blog post</a> /
              <a href="https://github.com/eedavidwu/Resnet_Classification">pytorch code</a>/
			  <a href="https://github.com/eedavidwu/yolov3_worker">darknet code</a>
              <p>
			  This project is to do small object detection on workers and classification on their wearing. 
			  I use the yolov3 and multi-task resnet34 as the baseline to detect whether workers wearing safety helmet and safety clothes. 
			  </p>
			  <p>
			  Besides, I draw the heatmap and add some constrains on the attention of the network to make higher mAP. This net has a mAP with 71% in workers' detection and 91% accracy with classification of their wearing.
				</p>
			</td>
          </tr>
		  
		  <!--3rd- Project: AUV-->
		<tr onmouseout="AUV_PRO_stop()" onmouseover="AUV_PRO_start()" bgcolor="#ECF5FF">
		
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='AUV_project_image'><img src='images/fishery_AUV.jpg'></div>
                <img src='images/AUV.jpg'>
              </div>
              <script type="text/javascript">
                function AUV_PRO_start() {
                  document.getElementById('AUV_project_image').style.opacity = "1";
                }

                function AUV_PRO_stop() {
                  document.getElementById('AUV_project_image').style.opacity = "0";
                }
                AUV_PRO_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><font color='#0066CC'>Visual Intelligence and Control of AUV</font></papertitle>
              <br>
              <strong>Haotian Wu</strong>, Liwei Kou, Shimin He
              <br>
              <em>Zhejiang Univerisy</em>, 2018.10-2019.06
              <br>
              <a href="https://youtu.be/tDh8MKWqdIM">video_fishery_AUV</a> /
              <a href="https://youtu.be/i9fgLTIMUnQ">video_AUV</a> 
              
              <p>
			  Develop a mini-AUV for fishery project with object detection and depth prediction. 
			  </p>
              <p>
			  Experiments in tracking and filters for the AUV.		  
			  </p>

            </td>
          </tr>
          <!--4th- Project: mobile robot-->
		<tr onmouseout="mobile_stop()" onmouseover="mobile_start()" bgcolor="#ECF5FF">
		
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mobile_image'><img src='images/car_cor.jpg'></div>
                <img src='images/mobile.jpg'>
              </div>
              <script type="text/javascript">
                function mobile_start() {
                  document.getElementById('mobile_image').style.opacity = "1";
                }

                function mobile_stop() {
                  document.getElementById('mobile_image').style.opacity = "0";
                }
                mobile_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle> <font color='#0066CC'> Navigation and Motion Control of Vision based Mobile Robot </font></papertitle>
             
              <br>
              <strong>Haotian Wu</strong>
              <br>
              <em>Zhejiang Univerisity</em>, 2017
              <br>
              <a href="https://arxiv.org/abs/1806.04171">video</a>/
			  <a href="https://github.com/eedavidwu/Mobile_robot_interface/">Interface design</a> /
			  <a href="https://github.com/eedavidwu/Mobile_robot_interface/">Code</a>
              <p>
			  This project researches on the navigation and motion control of mobile robot based on vision and IMU.
				</p>
			<p>
			  The main work in this project includes: hardware design, obtaining the location of mobile robot, control algorithm, correction of motion deviation, path planning and human-computer interaction.
				</p>
            </td>
          </tr>
         <!--5th 2 HhuaWei RF  -->
		<tr bgcolor="#ECF5FF">
		
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/rf_predict.jpg'>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
          
                <papertitle><font color='#0066CC'>Prediction of users' churn and portrait</font></papertitle>
          
              <br>
              Haotian Wu
              <br>
              <em>Huawei Research Institute, 2012 Laboratories</em>, 2016.12 -- 2017.06
              <br>
              <p>
			  Build the model to predict the user churn based on random forest algorithm
			  </p>
			  
              <p>
				Draw the portraits of users based on their bills and habits data. 
			  </p>
            </td>
          </tr>
		 <!--6th 2 wheel car -->
		<tr bgcolor="#ECF5FF">
		
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/car.jpg'>
			</td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle><font color='#0066CC'>Two-Wheel Self Balancing Smart Car</font></papertitle>

              <br>
              <strong>Haotian Wu</strong>
              
              <br>
              <em>Zhejiang Univerisity</em>, 2016
              <br>
              <a href="https://arxiv.org/abs/1806.04171">video</a> 
              <p>
			  This project uses gyroscope and accelerator to get the information of vehicle's pose, which is used to predict the pose in the motion by Kalman filter algorithm.
			  Photoelectric Sensor is used to obtain path information, and PID algorithm is used to realize the closed-loop control of two independent DC motors.
			  </p>
            </td>
          </tr>
		  
		</tbody>
	     </table>
	



             <p>
          <br>
				</p>
 
		

<!--Patents part-->		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>
           <tr bgcolor="#FFF8D7">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Patents</heading>
              <p>
                As the results of many projects I did, some of them results in paper, other results in patented patents (in Chinese):
              </p>
            </td>
            </tr>
		</tbody>
        </table>
		
	<!--Patents List-->		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>

		<!--1st patent-->
		<tr bgcolor="#F0FFF0">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/coor_robot.jpg'>
            </td>
			
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><font color='#0066CC'>Cooperative System of Indoor Mobile Robot Based on QR code and Fuzzy Algorithm</font></papertitle>
              <br>
              <strong>Haotian Wu</strong>, Kaijian Liu, Tianchen Zhang, &nbsp; &nbsp; Status: <strong> Patented</strong>
			  <br>Patent ID: No.201610964289.8
			  <br>
              <a href="./data/201610964289.8.jpg"> Authorization Document</a> /
			  <a href="http://www2.soopat.com/Patent/201610964289">Patent Information</a>
              <p>
			  The invention proposes an indoor multi-mobile robot cooperation system.  The path planning is based on QR-code. The motion correction is based on fuzzy PID.
			  </p>
			  <p>
			  The system has low positioning cost and can realize efficient path planning, which makes multi-robots work together, avoid conflicts and work efficient.
			  </p>
            </td>
		</tr>
		
		<!--2nd patent-->
		<tr bgcolor="#F0FFF0">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/algri_robot.jpg'>
            </td>
			
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><font color='#0066CC'>Agricultural mobile robot based on Random Forest</font></papertitle>

              <br>
              Zhiyun Lin, <strong>Haotian Wu</strong>, Junjie Liu, &nbsp; &nbsp; Status: &nbsp; <strong> Patented</strong> 
			  <br>Patent ID: No. 201710367405.2
			  <br>
			
              <a href="./data/201710367405.2.jpg"> Authorization Document</a> /
              <a href="http://www2.soopat.com/Patent/201710367405">Patent Information</a>
              
              <p>The invention proposes a mobile robot based on random forest and QR-code. The robot uses QR code for localization and predicts the opration by random forest.</p>
			  <p>The goal of the robot is to achieve agricultural automation and reduce manual participation.</p>
			</td>
		</tr>
		
        
		<!--3rd patent-->
	<tr bgcolor="#F0FFF0">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/water_saving.jpg'>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><font color='#0066CC'>Smart Water-saving Faucet Based on Neural Network and SCM</font></papertitle>
              <br>
              <strong>Haotian Wu</strong>, &nbsp; &nbsp; &nbsp; &nbsp; Status: <strong> Patented</strong>
			  <br>Patent ID: No.201610135909.7
              <br>
              <a href="./data/2016101359097.jpg"> Authorization Document</a> /
              <a href="http://www2.soopat.com/Patent/201610135909">Patent Documents</a>
              <p>The invention proposes an intelligent water-saving faucet based on neural network. This faucet can predict the needs of water temperature and meet people's needs quickly without realeasing too much cold water, which can avoid the waste of water resources  .</p>
            
			  </td>
		</tr>

		
		<!--4th patent-->
	<tr bgcolor="#F0FFF0">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/self_corre.jpg'>
            </td>
			
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><font color='#0066CC'>Self-correction algorithm based on attitude detection and special landmark</font></papertitle>
              <br>
              Zhiyun Lin, <strong>Haotian Wu</strong>, Kaijian Liu,&nbsp; &nbsp; Status: <strong> Patented</strong>
			  <br>Patent ID: No.201710378237.7
			  <br>
			  
              <a href="./data/201710378237.7.jpg"> Authorization Document</a> /
			  <a href="http://www2.soopat.com/Patent/201710378237">Patent Information</a>
              <p>The invention proposes a mobile robot based on random forest and QR-code. The robot uses QR code for localization and predicts the opration by random forest.</p>
			  <p>The goal of the robot is to achieve agricultural automation and reduce manual participation.</p>
            </td>
		</tr>
		
				<!--5th patent-->
	<tr bgcolor="#F0FFF0">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/patent_fisher.jpg'>
            </td>
			
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><font color='#0066CC'>A fishery growth monitoring system</font></papertitle>
              <br>
              Ji Xiang, <strong>Haotian Wu</strong>,&nbsp; &nbsp; Status: <strong> Under effective examination</strong>
			  <br>Patent ID: No.202010436332.X
			  <br>
			  
              <a href="./data/patent_fishery.pdf"> Authorization Document</a> /
			  <a href="./data/patent_fishery.pdf">Patent Information</a>
              <p>The invention proposes a fishery growth monitoring system based on YOLO and SGBM. </p>
			  <p>The goal of the system is to predict and monitor the fishery growth situation.</p>
            </td>
		</tr>
		

        
		
				
		</tbody>
		</table>
             <p>
          <br>
				</p>
 
   
<!--Honour-->		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>
      <tr bgcolor="#FFF8D7">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Awards and Honors</heading>
              <p>
               <strong>National Scholarship for Postgraduate Student</strong>
               <br>&emsp; 2018 - 2019 academic year
			  </p>
			  
           <p>
				</p>
			  
			  <p>
               <strong>		Excellent Postgraduate Students' Award</strong>
               <br>&emsp; awarded in 2020 graduation
			 </p>
				
                <p>
               <strong>Postgraduate Studentship in Imperial College London</strong>
               <br>&emsp; 2017 - 2018 academic year
               <br>&emsp; Cover the maintance fees in the academic year
				</p>
   
                <p>
				</p>
                <p>
               <strong>Outstanding undergraduate thesis in Zhejiang University</strong>
               <br>&emsp; 2016 - 2017 academic year
               <br>&emsp; Bachelor graduate project is awarded outstanding achievements award
				</p>
        
                <p>
				</p>
               
			   <p>
               <strong>The Scholarship for Academic Excellence in Zhejiang University</strong>
               <br>&emsp; 2019 - 2020 master academic year
			   <br>&emsp; 2018 - 2019 master academic year
			   <br>&emsp; 2015 - 2016 bachelor academic year
               <br>&emsp; 2014 - 2015 bachelor academic year
				</p>
        
                        <p>
				</p>
                        <p>
               <strong>The Scholarship for Outstanding Students in Zhejiang University</strong>.
			    <br>&emsp; 2019 - 2020 master academic year
               <br>&emsp; 2018 - 2019 master academic year
			   <br>&emsp; 2015 - 2016 bachelor academic year
               <br>&emsp; 2014 - 2015 bachelor academic year
				</p>

            </td>
        </tr>
       </tbody>
	   </table> 

                <p>
          <br>
				</p>
	
<!--Reference-->		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>
      <tr bgcolor="#FFF8D7">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Referee and Supervisor</heading>
              <p>
                My sincere gratitude goes to my two supervisors in Imperial College London: <a href="https://www.imperial.ac.uk/people/r.vinter"> Prof. Richard Vinter</a>  and  <a href="http://www.imperial.ac.uk/people/a.astolfi">Prof. Alessandro Astolfi</a>. I am very grateful for their kindest help.
				</p>
<p>
Many thanks to my supervisors in Zhejiang University: <a href="https://scholar.google.com/citations?user=ic9y2dIAAAAJ&hl=zh-CN">Prof. Zhiyun Lin</a> and <a href="https://scholar.google.com/citations?user=rgwDYosAAAAJ&hl=zh-CN&oi=ao">Prof. Ji Xiang</a>
</p>
            </td>
        </tr>
       </tbody>
	   </table>
  
              <p>
          <br>
				</p>
        		 	
<!--Acknogement-->		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>
      <tr bgcolor="#FFF8D7">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Acknowledgement</heading>
              <p>
               Some achievements would not have been the same without the support and coropration with my teammate in the lab. Grateful to some works from Dr. Liwei Kou, Shimin He, Feiyang Suo.
				</p>

            </td>
        </tr>
       </tbody>
	   </table>



	
		 	
		</tbody>  
		</table>






	</td>
    </tr>
</tbody>
</table>
</body>
</html>
